{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual_rl_method1_final.py\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import socket\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CONFIG = {\n",
    "    \"STATE_DIM\": 8,\n",
    "    \"ACTION_DIM\": 1,\n",
    "    \"HIDDEN\": 256,\n",
    "    \"LR\": 3e-4,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"TAU\": 0.005,\n",
    "    \"AUTO_ENTROPY\": True,\n",
    "\n",
    "    # Residual limits (MPa)\n",
    "    \"R_MIN\": -0.1,\n",
    "    \"R_MAX\":  0.1,\n",
    "    \"R_SLEW_PER_20MS\": 0.02,      # |Δresidual| limit per 20ms tick\n",
    "\n",
    "    # Scheduling\n",
    "    \"TICK_SEC\": 0.020,\n",
    "    \"TICK_TOL\": 0.003,\n",
    "\n",
    "    # Training\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"REPLAY_WARMUP\": 1000,\n",
    "\n",
    "    # Networking\n",
    "    \"HOST\": \"localhost\",\n",
    "    \"PORT\": 8888,\n",
    "    \"RECV_TIMEOUT_SEC\": 0.5,\n",
    "    \"COMM_FAIL_MAX\": 3,\n",
    "\n",
    "    # Episode\n",
    "    \"EPISODES\": 500,\n",
    "    \"MAX_EPISODE_STEPS\": 10000,   # ~10s if 1kHz sensor loop\n",
    "\n",
    "    # Safety / Reward shaping\n",
    "    \"MAX_FORCE_ERR\": 15.0,\n",
    "    \"MAX_PRESS_DELTA\": 0.05,\n",
    "\n",
    "    # Logging\n",
    "    \"LOG_EVERY_CTRL\": 50,         # every 50 control updates\n",
    "    \"SAVE_THRESH_FREQ\": 50,\n",
    "    \"SAVE_THRESH_PCT\": 90,\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Utils: seed\n",
    "# =========================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# =========================\n",
    "# SAC Models\n",
    "# =========================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = torch.clamp(self.log_std_head(x), self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action, log_prob.sum(1, keepdim=True)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q1_fc3 = nn.Linear(hidden_dim, 1)\n",
    "        self.q2_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q2_fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        q1 = F.relu(self.q1_fc1(sa))\n",
    "        q1 = F.relu(self.q1_fc2(q1))\n",
    "        q1 = self.q1_fc3(q1)\n",
    "        q2 = F.relu(self.q2_fc1(sa))\n",
    "        q2 = F.relu(self.q2_fc2(q2))\n",
    "        q2 = self.q2_fc3(q2)\n",
    "        return q1, q2\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class ResidualSACAgent:\n",
    "    def __init__(self, cfg=CONFIG):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        s_dim, a_dim, hidden = cfg[\"STATE_DIM\"], cfg[\"ACTION_DIM\"], cfg[\"HIDDEN\"]\n",
    "        self.gamma, self.tau = cfg[\"GAMMA\"], cfg[\"TAU\"]\n",
    "        self.alpha = 0.2\n",
    "        self.auto_entropy_tuning = cfg[\"AUTO_ENTROPY\"]\n",
    "\n",
    "        self.actor = Actor(s_dim, a_dim, hidden).to(self.device)\n",
    "        self.critic = Critic(s_dim, a_dim, hidden).to(self.device)\n",
    "        self.critic_target = Critic(s_dim, a_dim, hidden).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=cfg[\"LR\"])\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=cfg[\"LR\"])\n",
    "\n",
    "        if self.auto_entropy_tuning:\n",
    "            self.target_entropy = -torch.prod(torch.tensor([a_dim], device=self.device)).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_opt = optim.Adam([self.log_alpha], lr=cfg[\"LR\"])\n",
    "\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.total_steps = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if evaluate:\n",
    "                mean, _ = self.actor(state)\n",
    "                action = torch.tanh(mean)\n",
    "            else:\n",
    "                action, _ = self.actor.sample(state)\n",
    "        action = action.cpu().numpy().flatten()\n",
    "        # scale to residual MPa range [-0.1, 0.1]\n",
    "        return float(action[0] * (self.cfg[\"R_MAX\"]))\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # normalize back to [-1,1] for critic input consistency\n",
    "        norm_action = action / self.cfg[\"R_MAX\"]\n",
    "        self.replay.push(state, norm_action, reward, next_state, done)\n",
    "\n",
    "    def update_parameters(self, batch_size=None):\n",
    "        bs = batch_size or self.cfg[\"BATCH_SIZE\"]\n",
    "        if len(self.replay) < bs: return\n",
    "        s, a, r, ns, d = self.replay.sample(bs)\n",
    "        s = torch.FloatTensor(s).to(self.device)\n",
    "        a = torch.FloatTensor(a).to(self.device)\n",
    "        r = torch.FloatTensor(r).unsqueeze(1).to(self.device)\n",
    "        ns = torch.FloatTensor(ns).to(self.device)\n",
    "        d = torch.FloatTensor(d).unsqueeze(1).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            na, nlogp = self.actor.sample(ns)\n",
    "            q1n, q2n = self.critic_target(ns, na)\n",
    "            min_qn = torch.min(q1n, q2n) - self.alpha * nlogp\n",
    "            y = r + (1 - d) * self.gamma * min_qn\n",
    "\n",
    "        q1, q2 = self.critic(s, a)\n",
    "        q_loss = F.mse_loss(q1, y) + F.mse_loss(q2, y)\n",
    "        self.critic_opt.zero_grad()\n",
    "        q_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        pi, logp = self.actor.sample(s)\n",
    "        q1_pi, q2_pi = self.critic(s, pi)\n",
    "        min_q_pi = torch.min(q1_pi, q2_pi)\n",
    "        pi_loss = ((self.alpha * logp) - min_q_pi).mean()\n",
    "        self.actor_opt.zero_grad()\n",
    "        pi_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        if self.auto_entropy_tuning:\n",
    "            a_loss = -(self.log_alpha * (logp + self.target_entropy).detach()).mean()\n",
    "            self.alpha_opt.zero_grad(); a_loss.backward(); self.alpha_opt.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        # soft update\n",
    "        with torch.no_grad():\n",
    "            for tp, lp in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                tp.data.copy_(self.tau * lp.data + (1 - self.tau) * tp.data)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            \"actor\": self.actor.state_dict(),\n",
    "            \"critic\": self.critic.state_dict(),\n",
    "            \"critic_target\": self.critic_target.state_dict(),\n",
    "            \"actor_opt\": self.actor_opt.state_dict(),\n",
    "            \"critic_opt\": self.critic_opt.state_dict(),\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"episode_rewards\": self.episode_rewards,\n",
    "        }, path)\n",
    "        print(f\"💾 Saved: {path}\")\n",
    "\n",
    "# =========================\n",
    "# TCP Communicator (residual only)\n",
    "# =========================\n",
    "class ResidualRLCommunicator:\n",
    "    def __init__(self, host, port, recv_timeout):\n",
    "        self.host, self.port = host, port\n",
    "        self.recv_timeout = recv_timeout\n",
    "        self.socket = None\n",
    "        self.conn = None\n",
    "        self.connected = False\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "            self.socket.bind((self.host, self.port))\n",
    "            self.socket.listen(1)\n",
    "            print(f\"🔗 Waiting for Robot PC on {self.host}:{self.port} ...\")\n",
    "            conn, addr = self.socket.accept()\n",
    "            conn.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
    "            conn.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n",
    "            conn.settimeout(self.recv_timeout)\n",
    "            print(f\"✅ Connected: {addr}\")\n",
    "            self.conn = conn\n",
    "            self.connected = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Connection error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _recv_exact(self, nbytes):\n",
    "        data = b''\n",
    "        while len(data) < nbytes:\n",
    "            chunk = self.conn.recv(nbytes - len(data))\n",
    "            if not chunk:\n",
    "                return None\n",
    "            data += chunk\n",
    "        return data\n",
    "\n",
    "    def receive_state(self):\n",
    "        try:\n",
    "            size_data = self.conn.recv(4)\n",
    "            if not size_data:\n",
    "                return None\n",
    "            data_size = int.from_bytes(size_data, \"little\")\n",
    "            data = self._recv_exact(data_size)\n",
    "            if data is None:\n",
    "                return None\n",
    "            d = json.loads(data.decode(\"utf-8\"))\n",
    "            state = np.array([\n",
    "                d[\"current_force\"],   # 0\n",
    "                d[\"target_force\"],    # 1\n",
    "                d[\"force_error\"],     # 2\n",
    "                d[\"force_error_dot\"], # 3\n",
    "                d[\"force_error_int\"], # 4\n",
    "                d[\"current_pressure\"],# 5\n",
    "                d[\"pi_output\"],       # 6\n",
    "                d[\"sander_active\"]    # 7\n",
    "            ], dtype=np.float32)\n",
    "            rl_flag = bool(d[\"sander_active\"])\n",
    "            return state, rl_flag\n",
    "        except socket.timeout:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error receiving state: {e}\")\n",
    "            return None\n",
    "\n",
    "    def send_residual(self, rl_residual, episode_done, timing_accurate):\n",
    "        try:\n",
    "            payload = {\n",
    "                \"rl_residual\": float(rl_residual),\n",
    "                \"episode_done\": bool(episode_done),\n",
    "                \"timing_accurate\": bool(timing_accurate),\n",
    "            }\n",
    "            data = json.dumps(payload).encode(\"utf-8\")\n",
    "            self.conn.send(len(data).to_bytes(4, \"little\"))\n",
    "            self.conn.send(data)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error sending residual: {e}\")\n",
    "            return False\n",
    "\n",
    "    def send_reset(self):\n",
    "        try:\n",
    "            payload = {\"command\": \"reset_episode\"}\n",
    "            data = json.dumps(payload).encode(\"utf-8\")\n",
    "            self.conn.send(len(data).to_bytes(4, \"little\"))\n",
    "            self.conn.send(data)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error sending reset: {e}\")\n",
    "            return False\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            if self.conn: self.conn.close()\n",
    "            if self.socket: self.socket.close()\n",
    "        finally:\n",
    "            self.connected = False\n",
    "            print(\"🔌 Communication closed\")\n",
    "\n",
    "# =========================\n",
    "# Environment (Method 1)\n",
    "# =========================\n",
    "class PneumaticPolishingEnvironment:\n",
    "    def __init__(self, cfg=CONFIG):\n",
    "        self.cfg = cfg\n",
    "        self.agent = ResidualSACAgent(cfg)\n",
    "        self.comm = ResidualRLCommunicator(cfg[\"HOST\"], cfg[\"PORT\"], cfg[\"RECV_TIMEOUT_SEC\"])\n",
    "\n",
    "        # scheduler\n",
    "        self.last_tick = None\n",
    "\n",
    "        # residual limiter\n",
    "        self.prev_residual = 0.0\n",
    "\n",
    "        # episode stats\n",
    "        self.episode_step = 0\n",
    "        self.max_episode_steps = cfg[\"MAX_EPISODE_STEPS\"]\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.best_episode_reward = -float(\"inf\")\n",
    "        self.best_agent_episode = -1\n",
    "\n",
    "        # RL activity monitor\n",
    "        self.rl_inactive_count = 0\n",
    "        self.max_rl_inactive_steps = 250\n",
    "        self.rl_active_in_episode = False\n",
    "        self.total_rl_active_steps = 0\n",
    "\n",
    "        # comm fail\n",
    "        self.fail_count = 0\n",
    "        self.FAIL_MAX = cfg[\"COMM_FAIL_MAX\"]\n",
    "\n",
    "        # logging\n",
    "        self.control_updates = 0\n",
    "\n",
    "    # ---- scheduler ----\n",
    "    def should_send_now(self):\n",
    "        now = time.time()\n",
    "        if self.last_tick is None:\n",
    "            self.last_tick = now\n",
    "            return True, True\n",
    "        dt = now - self.last_tick\n",
    "        if dt >= self.cfg[\"TICK_SEC\"] - 1e-3:\n",
    "            is_exact = abs(dt - self.cfg[\"TICK_SEC\"]) <= self.cfg[\"TICK_TOL\"]\n",
    "            self.last_tick = now\n",
    "            return True, is_exact\n",
    "        return False, False\n",
    "\n",
    "    # ---- residual limiter ----\n",
    "    def limit_residual(self, r):\n",
    "        r = float(np.clip(r, self.cfg[\"R_MIN\"], self.cfg[\"R_MAX\"]))\n",
    "        delta = np.clip(r - self.prev_residual, -self.cfg[\"R_SLEW_PER_20MS\"], self.cfg[\"R_SLEW_PER_20MS\"])\n",
    "        r_limited = self.prev_residual + delta\n",
    "        self.prev_residual = r_limited\n",
    "        return r_limited\n",
    "\n",
    "    # ---- reward / done ----\n",
    "    def calculate_reward(self, state, action_residual):\n",
    "        current_force, target_force = state[0], state[1]\n",
    "        force_err = abs(current_force - target_force)\n",
    "        sander_active = bool(state[7])\n",
    "        residual_change = abs(action_residual - self.prev_residual)\n",
    "        # 1) tracking\n",
    "        reward = -(force_err / self.cfg[\"MAX_FORCE_ERR\"])\n",
    "        if force_err < 1.0: reward += 0.5\n",
    "        # 2) smoothness\n",
    "        smooth_w = 0.2 if sander_active else 0.3\n",
    "        reward += -smooth_w * (residual_change / self.cfg[\"MAX_PRESS_DELTA\"])\n",
    "        # 3) safety\n",
    "        if current_force > 80.0: reward += -5.0\n",
    "        # 4) residual magnitude penalty\n",
    "        reward += -0.1 * abs(action_residual)\n",
    "        return float(reward)\n",
    "\n",
    "    def is_done(self, state):\n",
    "        if self.episode_step >= self.max_episode_steps: return True\n",
    "        if state[0] > 100.0: return True  # force safety\n",
    "        return False\n",
    "\n",
    "    # ---- RL activity monitor ----\n",
    "    def check_rl_status(self, rl_flag):\n",
    "        if rl_flag:\n",
    "            self.rl_inactive_count = 0\n",
    "            self.rl_active_in_episode = True\n",
    "            self.total_rl_active_steps += 1\n",
    "            return \"active\"\n",
    "        else:\n",
    "            self.rl_inactive_count += 1\n",
    "            if self.rl_inactive_count >= self.max_rl_inactive_steps:\n",
    "                print(\"❌ RL inactive too long → terminate episode\")\n",
    "                return \"terminate\"\n",
    "            return \"inactive\"\n",
    "\n",
    "    # ---- episode helpers ----\n",
    "    def reset_episode(self):\n",
    "        self.prev_residual = 0.0\n",
    "        self.episode_step = 0\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.control_updates = 0\n",
    "        self.rl_inactive_count = 0\n",
    "        self.rl_active_in_episode = False\n",
    "        self.last_tick = None\n",
    "        ok = self.comm.send_reset()\n",
    "        if ok:\n",
    "            print(\"\\n--- Episode Reset ---\")\n",
    "            print(\"Robot PC: 1kHz PI running, will add RL residual (held) each tick.\")\n",
    "        else:\n",
    "            print(\"⚠️ Reset signal failed (continuing).\")\n",
    "        return ok\n",
    "\n",
    "    # ---- main loop ----\n",
    "    def run_training(self, episodes=None):\n",
    "        episodes = episodes or self.cfg[\"EPISODES\"]\n",
    "        if not self.comm.connect():\n",
    "            print(\"Failed to connect to Robot PC\")\n",
    "            return\n",
    "\n",
    "        print(\"🚀 TRUE Residual RL: RL sends residual at 50Hz, Robot sums PI(1kHz)+Residual.\")\n",
    "        for ep in range(episodes):\n",
    "            self.reset_episode()\n",
    "\n",
    "            prev_state = None\n",
    "            prev_action = None\n",
    "            prev_rl_flag = False\n",
    "\n",
    "            while True:\n",
    "                # receive latest state (blocking with timeout)\n",
    "                res = self.comm.receive_state()\n",
    "                if res is None:\n",
    "                    # no new state this timeout window; still respect tick schedule\n",
    "                    send_now, timing_ok = self.should_send_now()\n",
    "                    if not send_now:\n",
    "                        continue\n",
    "                    # cannot act without a state\n",
    "                    continue\n",
    "\n",
    "                state, rl_flag = res\n",
    "                self.episode_step += 1\n",
    "\n",
    "                send_now, timing_ok = self.should_send_now()\n",
    "                if not send_now:\n",
    "                    continue\n",
    "\n",
    "                # (1) store prev transition & learn on 50Hz boundary\n",
    "                if prev_state is not None and prev_rl_flag:\n",
    "                    reward = self.calculate_reward(prev_state, prev_action)\n",
    "                    done = self.is_done(state)  # next_state-based done\n",
    "                    self.agent.store_transition(prev_state, prev_action, reward, state, done)\n",
    "                    self.current_episode_reward += reward\n",
    "                    if len(self.agent.replay) > self.cfg[\"REPLAY_WARMUP\"]:\n",
    "                        self.agent.update_parameters(self.cfg[\"BATCH_SIZE\"])\n",
    "                    if done:\n",
    "                        self.comm.send_residual(0.0, True, timing_ok)\n",
    "                        break\n",
    "\n",
    "                # (2) compute residual (or zero if inactive)\n",
    "                rl_status = self.check_rl_status(rl_flag)\n",
    "                if rl_status == \"terminate\":\n",
    "                    print(\"Episode terminated due to prolonged RL inactivity\")\n",
    "                    break\n",
    "\n",
    "                if rl_flag:\n",
    "                    raw_res = self.agent.select_action(state, evaluate=False)\n",
    "                    rl_residual = self.limit_residual(raw_res)\n",
    "                    episode_done = self.is_done(state)\n",
    "                else:\n",
    "                    rl_residual = 0.0\n",
    "                    episode_done = False\n",
    "\n",
    "                # (3) send residual\n",
    "                ok = self.comm.send_residual(rl_residual, episode_done, timing_ok)\n",
    "                if not ok:\n",
    "                    self.fail_count += 1\n",
    "                    print(f\"⚠️ Send failed ({self.fail_count}/{self.FAIL_MAX})\")\n",
    "                    if self.fail_count >= self.FAIL_MAX:\n",
    "                        # advise PI-only fallback by ending episode with residual=0\n",
    "                        self.comm.send_residual(0.0, True, False)\n",
    "                        print(\"Comms degraded → advise PI-only fallback on Robot PC; ending episode.\")\n",
    "                        break\n",
    "                else:\n",
    "                    self.fail_count = 0\n",
    "\n",
    "                # (4) logging (light)\n",
    "                if self.control_updates % self.cfg[\"LOG_EVERY_CTRL\"] == 0:\n",
    "                    mode = \"RESIDUAL\" if rl_flag else \"PI-ONLY\"\n",
    "                    print(f\"[Ep {ep}] Step {self.episode_step} | {mode} | \"\n",
    "                          f\"F {state[0]:.1f}/{state[1]:.1f}N | \"\n",
    "                          f\"PI {state[6]:.3f}MPa | RL {rl_residual:.3f}MPa | \"\n",
    "                          f\"{'EXACT' if timing_ok else 'DELAY'}\")\n",
    "                self.control_updates += 1\n",
    "\n",
    "                # (5) keep for next transition\n",
    "                prev_state = state.copy()\n",
    "                prev_action = rl_residual\n",
    "                prev_rl_flag = rl_flag\n",
    "\n",
    "                if episode_done and rl_flag:\n",
    "                    break\n",
    "\n",
    "                if self.episode_step >= self.max_episode_steps:\n",
    "                    break\n",
    "\n",
    "            # ---- episode end ----\n",
    "            self.agent.episode_rewards.append(self.current_episode_reward)\n",
    "            if self.current_episode_reward > self.best_episode_reward:\n",
    "                self.best_episode_reward = self.current_episode_reward\n",
    "                self.best_agent_episode = ep\n",
    "                self.agent.save_model(f\"best_agent_episode_{ep}_reward_{self.best_episode_reward:.2f}.pth\")\n",
    "\n",
    "            # dynamic threshold saving (optional)\n",
    "            if ep % self.cfg[\"SAVE_THRESH_FREQ\"] == 0 and ep > 0:\n",
    "                recent = self.agent.episode_rewards[-self.cfg[\"SAVE_THRESH_FREQ\"]:]\n",
    "                th = np.percentile(recent, self.cfg[\"SAVE_THRESH_PCT\"])\n",
    "                if self.current_episode_reward >= th:\n",
    "                    self.agent.save_model(f\"high_perf_ep_{ep}_reward_{self.current_episode_reward:.2f}.pth\")\n",
    "\n",
    "            print(f\"Episode {ep} | Reward {self.current_episode_reward:.2f} | Best {self.best_episode_reward:.2f}\")\n",
    "\n",
    "        # final stats\n",
    "        print(\"\\n🎯 Training finished.\")\n",
    "        print(f\"Best Episode: {self.best_agent_episode}, Best Reward: {self.best_episode_reward:.2f}\")\n",
    "        self.comm.close()\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    env = PneumaticPolishingEnvironment(CONFIG)\n",
    "    try:\n",
    "        env.run_training(CONFIG[\"EPISODES\"])\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user.\")\n",
    "        env.comm.close()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        env.comm.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
