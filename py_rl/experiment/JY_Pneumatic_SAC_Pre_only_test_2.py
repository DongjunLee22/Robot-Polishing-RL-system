# residual_rl_method1_final.py - TEST VERSION
import os
import random
import time
import json
import socket
import struct
import threading
from collections import deque

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# =========================
# CONFIG - TEST VERSION
# =========================
CONFIG = {
    "STATE_DIM": 6,              # 6ê°œ ìƒíƒœ ë³€ìˆ˜ë¡œ ìˆ˜ì •
    "ACTION_DIM": 1,
    "HIDDEN": 256,
    "LR": 3e-4,
    "GAMMA": 0.99,
    "TAU": 0.005,
    "AUTO_ENTROPY": True,

    # Residual limits (MPa)
    "R_MIN": -0.1,
    "R_MAX":  0.1,
    "R_SLEW_PER_20MS": 0.02,     # |Î”residual| limit per 20ms tick

    # Scheduling
    "TICK_SEC": 0.020,
    "TICK_TOL": 0.003,

    # Training - TEST VERSION
    "BATCH_SIZE": 64,             # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„
    "REPLAY_WARMUP": 100,         # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¤„ì„

    # Networking
    "HOST": "0.0.0.0",
    "PORT": 8888,
    "RECV_TIMEOUT_SEC": 0.5,
    "COMM_FAIL_MAX": 3,

    # Episode - TEST VERSION
    "EPISODES": 10,               # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ì—í”¼ì†Œë“œ
    "MAX_EPISODE_STEPS": 2000,    # í…ŒìŠ¤íŠ¸ìš©: 40ì´ˆ (2000 ìŠ¤í…)

    # Safety / Reward shaping
    "MAX_FORCE_ERR": 15.0,
    "MAX_PRESS_DELTA": 0.05,

    # Logging
    "LOG_EVERY_CTRL": 25,         # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë” ìì£¼ ë¡œê¹…
    "SAVE_THRESH_FREQ": 5,        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë” ìì£¼ ì €ì¥
    "SAVE_THRESH_PCT": 80,        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚®ì¶¤
    
    # Model saving
    "MODEL_SAVE_DIR": "saved_agents",  # ëª¨ë¸ ì €ì¥ ì „ìš© í´ë”
}

# =========================
# Utils: seed
# =========================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# =========================
# SAC Models
# =========================
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):
        super().__init__()
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        mean = self.mean_head(x)
        log_std = torch.clamp(self.log_std_head(x), self.log_std_min, self.log_std_max)
        return mean, log_std

    def sample(self, state):
        mean, log_std = self.forward(state)
        std = log_std.exp()
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        action = torch.tanh(x_t)
        
        # log_prob ê³„ì‚° - ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìˆ˜ì •
        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob_sum = log_prob.sum(1, keepdim=True)  # (batch_size, 1) ì°¨ì›
        
        return action, log_prob_sum

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super().__init__()
        self.q1_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc3 = nn.Linear(hidden_dim, 1)
        self.q2_fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        sa = torch.cat([state, action], 1)
        q1 = F.relu(self.q1_fc1(sa))
        q1 = F.relu(self.q1_fc2(q1))
        q1 = self.q1_fc3(q1)
        q2 = F.relu(self.q2_fc1(sa))
        q2 = F.relu(self.q2_fc2(q2))
        q2 = self.q2_fc3(q2)
        return q1, q2

class ReplayBuffer:
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        
        # actionì„ (batch_size, 1) ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        if action.ndim == 1:
            action = action.reshape(-1, 1)
        
        return state, action, reward, next_state, done
    def __len__(self):
        return len(self.buffer)

class ResidualSACAgent:
    def __init__(self, cfg=CONFIG):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        s_dim, a_dim, hidden = cfg["STATE_DIM"], cfg["ACTION_DIM"], cfg["HIDDEN"]
        self.gamma, self.tau = cfg["GAMMA"], cfg["TAU"]
        self.alpha = 0.2
        self.auto_entropy_tuning = cfg["AUTO_ENTROPY"]

        self.actor = Actor(s_dim, a_dim, hidden).to(self.device)
        self.critic = Critic(s_dim, a_dim, hidden).to(self.device)
        self.critic_target = Critic(s_dim, a_dim, hidden).to(self.device)
        self.critic_target.load_state_dict(self.critic.state_dict())

        self.actor_opt = optim.Adam(self.actor.parameters(), lr=cfg["LR"])
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=cfg["LR"])

        if self.auto_entropy_tuning:
            self.target_entropy = -torch.prod(torch.tensor([a_dim], device=self.device)).item()
            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
            self.alpha_opt = optim.Adam([self.log_alpha], lr=cfg["LR"])

        self.replay = ReplayBuffer()
        self.total_steps = 0
        self.episode_rewards = []

    def select_action(self, state, evaluate=False):
        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)
        
        with torch.no_grad():
            if evaluate:
                mean, _ = self.actor(state)
                action = torch.tanh(mean)
            else:
                action, log_prob = self.actor.sample(state)
        
        action = action.cpu().numpy().flatten()
        
        # scale to residual MPa range [-0.1, 0.1]
        return float(action[0] * (self.cfg["R_MAX"]))

    def store_transition(self, state, action, reward, next_state, done):
        # normalize back to [-1,1] for critic input consistency
        norm_action = action / self.cfg["R_MAX"]
        
        # ë¦¬í”Œë ˆì´ë²„í¼ ì €ì¥ ëª¨ë‹ˆí„°ë§
        self.replay.push(state, norm_action, reward, next_state, done)
        
        # ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ ì¶”ì  (ë””ë²„ê¹…ìš©)
        if len(self.replay) % 100 == 0:  # 100ê°œë§ˆë‹¤ ë¡œê¹…
            print(f"ğŸ“Š ReplayBuffer: {len(self.replay)} transitions stored")

    def update_parameters(self, batch_size=None):
        bs = batch_size or self.cfg["BATCH_SIZE"]
        if len(self.replay) < bs: return
        s, a, r, ns, d = self.replay.sample(bs)
        
        s = torch.FloatTensor(s).to(self.device)
        a = torch.FloatTensor(a).to(self.device)
        r = torch.FloatTensor(r).unsqueeze(1).to(self.device)
        ns = torch.FloatTensor(ns).to(self.device)
        d = torch.FloatTensor(d).unsqueeze(1).to(self.device)

        with torch.no_grad():
            na, nlogp = self.actor.sample(ns)
            # nlogpëŠ” ì´ë¯¸ (batch_size, 1) ì°¨ì›
            q1n, q2n = self.critic_target(ns, na)
            min_qn = torch.min(q1n, q2n) - self.alpha * nlogp
            y = r + (1 - d) * self.gamma * min_qn

        q1, q2 = self.critic(s, a)
        q_loss = F.mse_loss(q1, y) + F.mse_loss(q2, y)
        self.critic_opt.zero_grad()
        q_loss.backward()
        nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_opt.step()

        pi, logp = self.actor.sample(s)
        # logpëŠ” ì´ë¯¸ (batch_size, 1) ì°¨ì›
        q1_pi, q2_pi = self.critic(s, pi)
        min_q_pi = torch.min(q1_pi, q2_pi)
        pi_loss = ((self.alpha * logp) - min_q_pi).mean()
        self.actor_opt.zero_grad()
        pi_loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_opt.step()

        if self.auto_entropy_tuning:
            # logpëŠ” (batch_size, 1)ì´ë¯€ë¡œ squeeze(1)ë¡œ (batch_size,)ë¡œ ë³€í™˜
            logp_entropy = logp.squeeze(1)
            a_loss = -(self.log_alpha * (logp_entropy + self.target_entropy).detach()).mean()
            self.alpha_opt.zero_grad(); a_loss.backward(); self.alpha_opt.step()
            self.alpha = self.log_alpha.exp()

        # soft update
        with torch.no_grad():
            for tp, lp in zip(self.critic_target.parameters(), self.critic.parameters()):
                tp.data.copy_(self.tau * lp.data + (1 - self.tau) * tp.data)

    def save_model(self, path):
        torch.save({
            "actor": self.actor.state_dict(),
            "critic": self.critic.state_dict(),
            "critic_target": self.critic_target.state_dict(),
            "actor_opt": self.actor_opt.state_dict(),
            "critic_opt": self.critic_opt.state_dict(),
            "total_steps": self.total_steps,
            "episode_rewards": self.episode_rewards,
        }, path)
        print(f"ğŸ’¾ Saved: {path}")

# =========================
# TCP Communicator (residual only)
# =========================
class ResidualRLCommunicator:
    def __init__(self, host, port, recv_timeout):
        self.host, self.port = host, port
        self.recv_timeout = recv_timeout
        self.socket = None
        self.conn = None
        self.connected = False
        
        # íŒ¨í‚· êµ¬ì¡° ì •ì˜ - 6ê°œ float + 1ê°œ boolë¡œ ìˆ˜ì •
        self.CPP_TO_PY_PACKET_FORMAT = ">HffffffBH"  # Big-Endian, 6ê°œ float + 1ê°œ bool
        self.CPP_TO_PY_PACKET_SIZE = 29  # 2+4Ã—6+1+2 = 29 bytes
        self.CPP_TO_PY_SOF = 0xAAAA
        
        self.PY_TO_CPP_PACKET_FORMAT = ">HfBBH"  # Big-Endian, 2+4+1+1+2 = 10 bytes
        self.PY_TO_CPP_PACKET_SIZE = 10  # 2+4+1+1+2 = 10 bytes
        self.PY_TO_CPP_SOF = 0xBBBB
        
        # ìˆ˜ì‹  ì“°ë ˆë“œ ê´€ë ¨
        self.latest_state = None
        self.latest_rl_flag = False
        self.receive_thread = None
        self.is_receiving = False
        self.state_lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        
        # ëª¨ë‹ˆí„°ë§ í†µê³„ ì¶”ê°€
        self.packets_received = 0
        self.packets_sent = 0
        self.checksum_errors = 0
        self.sof_errors = 0
        self.packet_size_errors = 0
        self.last_packet_time = None
        self.connection_start_time = None
        
        # ğŸ¯ 29ë°”ì´íŠ¸ ë°ì´í„° ìˆ˜ì‹  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¶”ê°€
        self.expected_packet_interval = 0.001  # 1ms (1000Hz)
        self.packet_receive_times = []  # ìµœê·¼ 100ê°œ íŒ¨í‚· ìˆ˜ì‹  ì‹œê°„
        self.packet_sequence_numbers = []  # íŒ¨í‚· ìˆœì„œ ë²ˆí˜¸ (ìµœê·¼ 100ê°œ)
        self.missed_packets = 0  # ëˆ„ë½ëœ íŒ¨í‚· ìˆ˜
        self.late_packets = 0  # ì§€ì—°ëœ íŒ¨í‚· ìˆ˜
        self.max_packet_history = 100  # ìµœëŒ€ ê¸°ë¡ ê°œìˆ˜

    def connect(self):
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.socket.bind((self.host, self.port))
            self.socket.listen(1)
            print(f"ğŸ”— Waiting for Robot PC on {self.host}:{self.port} ...")
            conn, addr = self.socket.accept()
            conn.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
            conn.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
            conn.settimeout(self.recv_timeout)
            print(f"âœ… Connected: {addr}")
            self.conn = conn
            self.connected = True
            
            # ì—°ê²° ì‹œê°„ ê¸°ë¡
            self.connection_start_time = time.time()
            
            # ì—°ê²° í›„ ìˆ˜ì‹  ì“°ë ˆë“œ ì‹œì‘
            self.start_receiving()
            return True
        except Exception as e:
            print(f"âŒ Connection error: {e}")
            return False

    def start_receiving(self):
        """1000Hzë¡œ ê³„ì† ìˆ˜ì‹ í•˜ëŠ” ë³„ë„ ì“°ë ˆë“œ ì‹œì‘"""
        self.is_receiving = True
        self.receive_thread = threading.Thread(target=self._receive_loop, daemon=True)
        self.receive_thread.start()
        print("ğŸ“¡ Started 1000Hz receiving thread")

    def _receive_loop(self):
        """1000Hzë¡œ ê³„ì† ìˆ˜ì‹ í•˜ëŠ” ë©”ì¸ ë£¨í”„"""
        while self.is_receiving:
            try:
                # non-blocking ìˆ˜ì‹  (1ms timeout)
                self.conn.settimeout(0.001)
                data = self._recv_exact(self.CPP_TO_PY_PACKET_SIZE)
                if data:
                    state, rl_flag = self._process_packet(data)
                    if state is not None:
                        with self.state_lock:
                            self.latest_state = state
                            self.latest_rl_flag = rl_flag
                            self.last_packet_time = time.time()
            except socket.timeout:
                continue
            except Exception as e:
                print(f"âš ï¸ Receive loop error: {e}")
                break
        print("ğŸ“¡ Receive loop terminated")

    def _recv_exact(self, nbytes):
        """ì •í™•íˆ në°”ì´íŠ¸ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ë°˜ë³µ ìˆ˜ì‹ """
        data = b''
        while len(data) < nbytes:
            chunk = self.conn.recv(nbytes - len(data))
            if not chunk:
                return None
            data += chunk
        return data

    def _process_packet(self, data):
        """ìˆ˜ì‹ ëœ íŒ¨í‚·ì„ ì²˜ë¦¬í•˜ì—¬ ìƒíƒœì™€ í”Œë˜ê·¸ ë°˜í™˜"""
        try:
            # 1. ê¸¸ì´ ê²€ì¦
            if len(data) != self.CPP_TO_PY_PACKET_SIZE:
                self.packet_size_errors += 1
                print(f"âš ï¸ [ê²½ê³ ] {self.CPP_TO_PY_PACKET_SIZE}Bê°€ ì•„ë‹Œ {len(data)}B ìˆ˜ì‹  (ì´ {self.packet_size_errors}íšŒ)")
                return None, False
            
            # 2. ì–¸íŒ¨í‚¹ - í¬ë§· ìˆ˜ì •
            try:
                (sof, current_force, target_force, force_error, force_error_dot, 
                 force_error_int, pi_output, sander_active, 
                 received_checksum) = struct.unpack(">HffffffBH", data)
            except struct.error as e:
                print(f"âš ï¸ [ì˜¤ë¥˜] struct.unpack ì‹¤íŒ¨: {e}")
                return None, False
            
            # 3. SOF ê²€ì¦
            if sof != self.CPP_TO_PY_SOF:
                self.sof_errors += 1
                print(f"âš ï¸ [ì˜¤ë¥˜] SOF ë¶ˆì¼ì¹˜: {hex(sof)} (ê¸°ëŒ€: {hex(self.CPP_TO_PY_SOF)}) (ì´ {self.sof_errors}íšŒ)")
                return None, False
            
            # 4. ì²´í¬ì„¬ ê²€ì¦ (CRC-16)
            calculated_checksum = self.calculate_crc16(data[:-2])
            
            if received_checksum != calculated_checksum:
                self.checksum_errors += 1
                print(f"âŒ [ì²´í¬ì„¬ ì˜¤ë¥˜] recv:{received_checksum} calc:{calculated_checksum} (ì´ {self.checksum_errors}íšŒ)")
                return None, False
            
            # 5. ìƒíƒœ ë°°ì—´ êµ¬ì„± - 6ê°œ ë³€ìˆ˜ë¡œ ìˆ˜ì •
            state = np.array([
                current_force,      # 0: RL_currentForceZ
                target_force,       # 1: RL_targetForceZ  
                force_error,        # 2: RL_forceZError
                force_error_dot,    # 3: RL_forceZErrordot
                force_error_int,    # 4: RL_forceZErrorintegral
                pi_output,          # 5: RL_pidFlag (float)
            ], dtype=np.float32)
            
            rl_flag = bool(sander_active)
            
            # 6. ìˆ˜ì‹  ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            self.packets_received += 1
            
            # ğŸ¯ 29ë°”ì´íŠ¸ ë°ì´í„° ìˆ˜ì‹  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
            current_time = time.time()
            
            # íŒ¨í‚· ìˆ˜ì‹  ì‹œê°„ ê¸°ë¡
            self.packet_receive_times.append(current_time)
            if len(self.packet_receive_times) > self.max_packet_history:
                self.packet_receive_times.pop(0)
            
            # íŒ¨í‚· ìˆœì„œ ë²ˆí˜¸ ê¸°ë¡
            self.packet_sequence_numbers.append(self.packets_received)
            if len(self.packet_sequence_numbers) > self.max_packet_history:
                self.packet_sequence_numbers.pop(0)
            
            # íŒ¨í‚· ê°„ê²© ë° ëˆ„ë½ ê²€ì‚¬ (ìµœì†Œ 2ê°œ íŒ¨í‚·ì´ ìˆì„ ë•Œ)
            if len(self.packet_receive_times) >= 2:
                last_interval = self.packet_receive_times[-1] - self.packet_receive_times[-2]
                
                # ì§€ì—°ëœ íŒ¨í‚· ê²€ì‚¬ (1msë³´ë‹¤ ëŠ¦ê²Œ ë„ì°©)
                if last_interval > self.expected_packet_interval * 1.5:  # 1.5ms ì´ìƒ
                    self.late_packets += 1
                
                # ëˆ„ë½ëœ íŒ¨í‚· ì¶”ì • (ê°„ê²©ì´ ë„ˆë¬´ í´ ë•Œ)
                expected_packets = int(last_interval / self.expected_packet_interval)
                if expected_packets > 1:
                    self.missed_packets += (expected_packets - 1)
            
            return state, rl_flag
            
        except Exception as e:
            print(f"âš ï¸ Packet processing error: {e}")
            return None, False

    def calculate_crc16(self, data: bytes) -> int:
        """CRC-16/MODBUS ì²´í¬ì„¬ ê³„ì‚° (C++ì™€ ë™ì¼)"""
        crc = 0xFFFF
        for byte in data:
            crc ^= byte
            for _ in range(8):
                if crc & 0x0001:
                    crc = (crc >> 1) ^ 0xA001
                else:
                    crc = crc >> 1
        return crc



    def get_latest_state(self):
        """ë©”ì¸ ë£¨í”„ì—ì„œ í˜¸ì¶œí•˜ì—¬ ìµœì‹  ìƒíƒœ ë°˜í™˜ (non-blocking)"""
        with self.state_lock:
            if self.latest_state is not None:
                # sander_active ìƒíƒœ ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê¹… ì¶”ê°€
                if hasattr(self, 'last_logged_rl_flag') and self.last_logged_rl_flag != self.latest_rl_flag:
                    print(f"ğŸ”„ RL Flag Changed: {self.last_logged_rl_flag} -> {self.latest_rl_flag}")
                    self.last_logged_rl_flag = self.latest_rl_flag
                elif not hasattr(self, 'last_logged_rl_flag'):
                    self.last_logged_rl_flag = self.latest_rl_flag
                    print(f"ğŸ”„ Initial RL Flag: {self.latest_rl_flag}")
                
                return self.latest_state.copy(), self.latest_rl_flag
        return None, False

    def send_residual(self, rl_residual, timing_accurate, episode_done):
        """50Hzë¡œ residual ì „ì†¡"""
        try:
            # 1. 10ë°”ì´íŠ¸ íŒ¨í‚· êµ¬ì„±
            # SOF(2) + rl_residual(4) + timing_accurate(1) + episode_done(1) + checksum(2)
            
            # 2. ì²´í¬ì„¬ ê³„ì‚°ìš© ë°ì´í„° (checksum ì œì™¸ ë¶€ë¶„)
            data_part = struct.pack(">HfBB", 
                                  self.PY_TO_CPP_SOF, 
                                  float(rl_residual), 
                                  int(timing_accurate), 
                                  int(episode_done))
            checksum = self.calculate_crc16(data_part)
            
            # 3. ìµœì¢… íŒ¨í‚· (SOF, float, unsigned char, unsigned char, checksum[uint16])
            final_packet = struct.pack(self.PY_TO_CPP_PACKET_FORMAT, 
                                     self.PY_TO_CPP_SOF, 
                                     float(rl_residual), 
                                     int(timing_accurate), 
                                     int(episode_done), 
                                     checksum)
            
            # 4. ì†¡ì‹ 
            self.conn.sendall(final_packet)
            
            # 5. ì†¡ì‹  ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            self.packets_sent += 1
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ Error sending residual: {e}")
            return False

    def send_reset(self):
        """ì—í”¼ì†Œë“œ ë¦¬ì…‹ ëª…ë ¹ ì „ì†¡ (binary protocol)"""
        try:
            # ğŸ¯ binary protocolë¡œ reset ì‹ í˜¸ ì „ì†¡
            # SOF(2) + reset_flag(1) + padding(3) + checksum(2) = 8 bytes
            reset_data = struct.pack(">HBxxxH", 0xCCCC, 1, 0)  # 0xCCCC = reset SOF
            checksum = self.calculate_crc16(reset_data[:-2])
            reset_packet = struct.pack(">HBxxxH", 0xCCCC, 1, checksum)
            
            self.conn.sendall(reset_packet)
            return True
        except Exception as e:
            print(f"âš ï¸ Error sending reset: {e}")
            return False

    def get_communication_stats(self):
        """í†µì‹  í†µê³„ ë°˜í™˜"""
        uptime = time.time() - self.connection_start_time if self.connection_start_time else 0
        
        # ğŸ¯ 29ë°”ì´íŠ¸ ë°ì´í„° ìˆ˜ì‹  í’ˆì§ˆ í†µê³„ ê³„ì‚°
        packet_quality_stats = {}
        if len(self.packet_receive_times) >= 2:
            # í‰ê·  íŒ¨í‚· ê°„ê²© ê³„ì‚°
            intervals = [self.packet_receive_times[i] - self.packet_receive_times[i-1] 
                        for i in range(1, len(self.packet_receive_times))]
            avg_interval = sum(intervals) / len(intervals)
            packet_quality_stats["avg_packet_interval_ms"] = avg_interval * 1000
            
            # íŒ¨í‚· ê°„ê²© í‘œì¤€í¸ì°¨ ê³„ì‚°
            interval_variance = sum((x - avg_interval) ** 2 for x in intervals) / len(intervals)
            packet_quality_stats["packet_interval_std_ms"] = (interval_variance ** 0.5) * 1000
            
            # ëˆ„ë½ëœ íŒ¨í‚· ë¹„ìœ¨ ê³„ì‚°
            expected_packets = uptime / self.expected_packet_interval
            packet_quality_stats["packet_loss_rate_percent"] = (self.missed_packets / expected_packets * 100) if expected_packets > 0 else 0
        
        return {
            "uptime_seconds": uptime,
            "packets_received": self.packets_received,
            "packets_sent": self.packets_sent,
            "checksum_errors": self.checksum_errors,
            "sof_errors": self.sof_errors,
            "packet_size_errors": self.packet_size_errors,
            "last_packet_time": self.last_packet_time,
            "receive_rate_hz": self.packets_received / uptime if uptime > 0 else 0,
            "send_rate_hz": self.packets_sent / uptime if uptime > 0 else 0,
            # ğŸ¯ 29ë°”ì´íŠ¸ ë°ì´í„° í’ˆì§ˆ í†µê³„ ì¶”ê°€
            "missed_packets": self.missed_packets,
            "late_packets": self.late_packets,
            **packet_quality_stats
        }

    def print_communication_stats(self):
        """í†µì‹  í†µê³„ ì¶œë ¥"""
        stats = self.get_communication_stats()
        print("\nğŸ“Š === COMMUNICATION STATISTICS ===")
        print(f"â±ï¸  Uptime: {stats['uptime_seconds']:.1f}s")
        print(f"ğŸ“¥ Packets Received: {stats['packets_received']}")
        print(f"ğŸ“¤ Packets Sent: {stats['packets_sent']}")
        print(f"ğŸ“¥ Receive Rate: {stats['receive_rate_hz']:.1f} Hz")
        print(f"ğŸ“¤ Send Rate: {stats['send_rate_hz']:.1f} Hz")
        print(f"âŒ Checksum Errors: {stats['checksum_errors']}")
        print(f"âš ï¸  SOF Errors: {stats['sof_errors']}")
        print(f"ğŸ“ Packet Size Errors: {stats['packet_size_errors']}")
        
        # ğŸ¯ 29ë°”ì´íŠ¸ ë°ì´í„° ìˆ˜ì‹  í’ˆì§ˆ ìƒì„¸ ì •ë³´
        if 'avg_packet_interval_ms' in stats:
            print(f"\nğŸ¯ === 29-BYTE DATA QUALITY ===")
            print(f"ğŸ“Š Avg Packet Interval: {stats['avg_packet_interval_ms']:.3f}ms (Target: 1.000ms)")
            print(f"ğŸ“Š Packet Interval Std: {stats['packet_interval_std_ms']:.3f}ms")
            print(f"ğŸ“Š Missed Packets: {stats['missed_packets']}")
            print(f"ğŸ“Š Late Packets: {stats['late_packets']}")
            print(f"ğŸ“Š Packet Loss Rate: {stats['packet_loss_rate_percent']:.2f}%")
            
            # í’ˆì§ˆ í‰ê°€
            if stats['packet_loss_rate_percent'] < 0.1:
                quality = "ğŸŸ¢ EXCELLENT"
            elif stats['packet_loss_rate_percent'] < 1.0:
                quality = "ğŸŸ¡ GOOD"
            elif stats['packet_loss_rate_percent'] < 5.0:
                quality = "ğŸŸ  FAIR"
            else:
                quality = "ğŸ”´ POOR"
            print(f"ğŸ“Š Overall Quality: {quality}")
        
        if stats['last_packet_time']:
            time_since_last = time.time() - stats['last_packet_time']
            print(f"\nğŸ• Last Packet: {time_since_last:.3f}s ago")
        print("=" * 40)

    def close(self):
        """ì—°ê²° ì¢…ë£Œ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìˆ˜ì‹  ì“°ë ˆë“œ ì¢…ë£Œ
            self.is_receiving = False
            if self.receive_thread and self.receive_thread.is_alive():
                self.receive_thread.join(timeout=1.0)
            
            # ì†Œì¼“ ì¢…ë£Œ
            if self.conn: 
                self.conn.close()
            if self.socket: 
                self.socket.close()
        finally:
            self.connected = False
            print("ğŸ”Œ Communication closed")

# =========================
# Environment (Method 1) - TEST VERSION
# =========================
class PneumaticPolishingEnvironment:
    def __init__(self, cfg=CONFIG):
        self.cfg = cfg
        self.agent = ResidualSACAgent(cfg)
        self.comm = ResidualRLCommunicator(cfg["HOST"], cfg["PORT"], cfg["RECV_TIMEOUT_SEC"])

        # scheduler
        self.last_tick = None

        # residual limiter
        self.prev_residual = 0.0

        # episode stats
        self.episode_step = 0
        self.max_episode_steps = cfg["MAX_EPISODE_STEPS"]
        self.current_episode_reward = 0.0
        self.best_episode_reward = -float("inf")
        self.best_agent_episode = -1

        # RL activity monitor
        self.rl_inactive_count = 0
        self.max_rl_inactive_steps = 250
        self.rl_active_in_episode = False
        self.total_rl_active_steps = 0

        # comm fail
        self.fail_count = 0
        self.FAIL_MAX = cfg["COMM_FAIL_MAX"]

        # logging
        self.control_updates = 0
        self.last_log_time = None  # ì‹œê°„ ê¸°ë°˜ ë¡œê¹…ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€

    # ---- scheduler ----
    def should_send_now(self):
        now = time.time()
        if self.last_tick is None:
            self.last_tick = now
            return True, True
        
        dt = now - self.last_tick
        
        # 50Hz = 20ms ê°„ê²©
        if dt >= self.cfg["TICK_SEC"]:
            is_exact = abs(dt - self.cfg["TICK_SEC"]) <= self.cfg["TICK_TOL"]
            self.last_tick = now
            
            # 50Hz íƒ€ì´ë° ë””ë²„ê¹… (1ì´ˆë§ˆë‹¤)
            if int(now) % 1 == 0 and int(self.last_tick) % 1 == 0:
                print(f"â±ï¸ Scheduler: dt={dt*1000:.1f}ms, exact={is_exact}")
            
            return True, is_exact
        
        return False, False

    # ---- residual limiter ----
    def limit_residual(self, r):
        r = float(np.clip(r, self.cfg["R_MIN"], self.cfg["R_MAX"]))
        delta = np.clip(r - self.prev_residual, -self.cfg["R_SLEW_PER_20MS"], self.cfg["R_SLEW_PER_20MS"])
        r_limited = self.prev_residual + delta
        self.prev_residual = r_limited
        return r_limited

    # ---- reward / done ----
    def calculate_reward(self, state, action_residual, rl_flag):
        current_force, target_force = state[0], state[1]
        force_err = abs(current_force - target_force)
        residual_change = abs(action_residual - self.prev_residual)
        # 1) tracking
        reward = -(force_err / self.cfg["MAX_FORCE_ERR"])
        if force_err < 1.0: reward += 0.5
        # 2) smoothness (rl_flagì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •)
        smooth_w = 0.2 if rl_flag else 0.3
        reward += -smooth_w * (residual_change / self.cfg["MAX_PRESS_DELTA"])
        # 3) safety
        if current_force > 80.0: reward += -5.0
        # 4) residual magnitude penalty
        reward += -0.1 * abs(action_residual)
        return float(reward)

    def is_done(self, state):
        # ğŸ¯ ì˜¤ì§ ìµœëŒ€ ìŠ¤í…ì— ë„ë‹¬í–ˆì„ ë•Œë§Œ True (ì—í”¼ì†Œë“œ ëê¹Œì§€ ì§„í–‰)
        if self.episode_step >= self.max_episode_steps: 
            return True
        
        # ğŸš¨ ì•ˆì „ì¥ì¹˜: ì ‘ì´‰ë ¥ì´ ê³¼ë„í•˜ê²Œ ë†’ì„ ë•Œë§Œ ì¢…ë£Œ
        if state[0] > 100.0: 
            print(f"âš ï¸ Safety: Force too high ({state[0]:.1f}N > 100N) - Episode terminated")
            return True
        
        # âœ… ë‹¤ë¥¸ ëª¨ë“  ê²½ìš°: ê³„ì† ì§„í–‰ (ëª©í‘œ ì ‘ì´‰ë ¥ ë‹¬ì„±í•´ë„ ê³„ì†)
        return False

    # ---- RL activity monitor ----
    def check_rl_status(self, rl_flag):
        if rl_flag:
            self.rl_inactive_count = 0
            self.rl_active_in_episode = True
            self.total_rl_active_steps += 1
            return "active"
        else:
            self.rl_inactive_count += 1
            if self.rl_inactive_count >= self.max_rl_inactive_steps:
                print("âŒ RL inactive too long â†’ terminate episode")
                return "terminate"
            return "inactive"

    # ---- episode helpers ----
    def reset_episode(self):
        self.prev_residual = 0.0
        self.episode_step = 0
        self.current_episode_reward = 0.0
        self.control_updates = 0
        self.rl_inactive_count = 0
        self.rl_active_in_episode = False
        self.last_tick = None
        self.last_log_time = None  # ë¡œê¹… ì‹œê°„ ë¦¬ì…‹
        ok = self.comm.send_reset()
        if ok:
            print("\n--- Episode Reset ---")
            print("Robot PC: 1kHz PI running, will add RL residual (held) each tick.")
        else:
            print("âš ï¸ Reset signal failed (continuing).")
        return ok

    # ---- main loop ----
    def run_training(self, episodes=None):
        episodes = episodes or self.cfg["EPISODES"]
        if not self.comm.connect():
            print("Failed to connect to Robot PC")
            return

        # saved_agents í´ë” ìƒì„±
        model_save_dir = self.cfg["MODEL_SAVE_DIR"]
        os.makedirs(model_save_dir, exist_ok=True)
        print(f"ğŸ“ Model save directory: {model_save_dir}")

        print("ğŸš€ TEST VERSION: TRUE Residual RL - 10 episodes, 100 seconds each")
        print("ğŸ“¡ RL sends residual at 50Hz, Robot sums PI(1kHz)+Residual.")
        print("â±ï¸  Each episode: 2,000 steps (40 seconds) - ëª©í‘œ ì ‘ì´‰ë ¥ ë‹¬ì„±í•´ë„ ê³„ì† ì§„í–‰")
        print("ğŸ¯ Episode completion: Only when max steps (2,000) reached")
        
        # ğŸ¯ RL Flagê°€ Trueê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ë¡œë´‡ì´ Control Step 2ì— ë„ë‹¬í•  ë•Œê¹Œì§€)
        print("\nğŸ”„ Waiting for Robot to reach Control Step 2 (RL activation)...")
        print("ğŸ“‹ Robot Control Sequence:")
        print("   Step 0: Robot descending to mold (position control)")
        print("   Step 1: Contact force stabilization (5 seconds)")
        print("   Step 2: Sander ON + PI control + RL activation")
        
        wait_start_time = time.time()
        while True:
            state, rl_flag = self.comm.get_latest_state()
            if rl_flag:  # sander_active = True
                wait_duration = time.time() - wait_start_time
                print(f"ğŸ¯ RL Activated! (waited {wait_duration:.1f}s)")
                print("ğŸš€ Starting RL episodes...")
                break
            
            # ëŒ€ê¸° ì¤‘ ìƒíƒœ í‘œì‹œ
            if state is not None:
                current_force = state[0]
                target_force = state[1]
                print(f"â³ Waiting... Current Force: {current_force:.1f}N, Target: {target_force:.1f}N", end='\r')
            
            time.sleep(0.1)  # 100ms ëŒ€ê¸°
            
            # ë¬´í•œ ëŒ€ê¸° ë°©ì§€ (5ë¶„ í›„ íƒ€ì„ì•„ì›ƒ)
            if time.time() - wait_start_time > 300:
                print(f"\nâš ï¸ Timeout! Robot didn't reach Control Step 2 in 5 minutes")
                print("ğŸ” Check robot control settings and physical contact")
                return
        
        # ì—í”¼ì†Œë“œë³„ í†µê³„
        episode_stats = []
        
        for ep in range(episodes):
            print(f"\nğŸ¬ === EPISODE {ep+1}/{episodes} START ===")
            
            # ğŸ¯ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ RL Flag ìƒíƒœ í™•ì¸
            episode_start_state, episode_start_rl_flag = self.comm.get_latest_state()
            if not episode_start_rl_flag:
                print(f"âš ï¸ Warning: RL Flag is False at episode {ep+1} start")
                print("ğŸ”„ Waiting for RL activation...")
                wait_start = time.time()
                while not episode_start_rl_flag:
                    episode_start_state, episode_start_rl_flag = self.comm.get_latest_state()
                    if time.time() - wait_start > 60:  # 1ë¶„ íƒ€ì„ì•„ì›ƒ
                        print(f"âš ï¸ Timeout waiting for RL activation in episode {ep+1}")
                        break
                    time.sleep(0.1)
            
            episode_start_time = time.time()
            self.reset_episode()

            # ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘ ì‹ í˜¸ ì „ì†¡
            print(f"ğŸ“¡ Starting episode {ep+1} - sending episode_done=False")
            self.comm.send_residual(0.0, True, False)

            prev_state = None
            prev_action = None
            prev_rl_flag = False
            
            # ì—í”¼ì†Œë“œë³„ í†µê³„
            episode_packets_received = 0
            episode_packets_sent = 0
            episode_rl_active_steps = 0

            while True:
                # ìˆ˜ì‹ ëœ ìµœì‹  ìƒíƒœ ê°€ì ¸ì˜¤ê¸° (non-blocking)
                res = self.comm.get_latest_state()
                if res[0] is None:
                    # ì•„ì§ ìˆ˜ì‹ ëœ ìƒíƒœê°€ ì—†ìŒ
                    send_now, timing_ok = self.should_send_now()
                    if not send_now:
                        continue
                    # ìƒíƒœê°€ ì—†ìœ¼ë©´ ëŒ€ê¸°
                    time.sleep(0.001)  # 1ms ëŒ€ê¸°
                    continue

                state, rl_flag = res
                episode_packets_received += 1

                send_now, timing_ok = self.should_send_now()
                if not send_now:
                    continue
                
                # ğŸ¯ 50Hz ê¸°ì¤€ìœ¼ë¡œë§Œ ìŠ¤í… ì¦ê°€ (1000Hzê°€ ì•„ë‹Œ)
                self.episode_step += 1

                # (1) store prev transition & learn on 50Hz boundary
                if prev_state is not None and prev_rl_flag:
                    reward = self.calculate_reward(prev_state, prev_action, prev_rl_flag)
                    done = self.is_done(state)  # next_state-based done
                    self.agent.store_transition(prev_state, prev_action, reward, state, done)
                    self.current_episode_reward += reward
                    if len(self.agent.replay) > self.cfg["REPLAY_WARMUP"]:
                        self.agent.update_parameters(self.cfg["BATCH_SIZE"])
                    
                    # ğŸ¯ done ì²´í¬ëŠ” ì•„ë˜ì—ì„œ í†µí•© ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)

                # (2) compute residual (or zero if inactive)
                rl_status = self.check_rl_status(rl_flag)
                if rl_status == "terminate":
                    print("Episode terminated due to prolonged RL inactivity")
                    break

                # ğŸ¯ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œì—ë§Œ episode_done = True ì „ì†¡
                if self.episode_step >= self.max_episode_steps:
                    episode_done = True
                    rl_residual = 0.0  # ì¢…ë£Œ ì‹œ residualì€ 0
                    print(f"ğŸ¯ Episode {ep+1} ending at step {self.episode_step} - sending episode_done=True")
                else:
                    episode_done = False
                    if rl_flag:
                        # ğŸ¯ 50Hzë¡œë§Œ select_action í˜¸ì¶œ (1000Hzê°€ ì•„ë‹Œ)
                        if send_now:  # send_nowê°€ Trueì¼ ë•Œë§Œ
                            raw_res = self.agent.select_action(state, evaluate=False)
                            rl_residual = self.limit_residual(raw_res)
                            episode_rl_active_steps += 1
                        else:
                            # send_nowê°€ Falseë©´ ì´ì „ residual ìœ ì§€
                            rl_residual = prev_action if prev_action is not None else 0.0
                    else:
                        rl_residual = 0.0

                # (3) send residual (50Hz)
                ok = self.comm.send_residual(rl_residual, timing_ok, episode_done)
                if not ok:
                    self.fail_count += 1
                    print(f"âš ï¸ Send failed ({self.fail_count}/{self.FAIL_MAX})")
                    if self.fail_count >= self.FAIL_MAX:
                        # advise PI-only fallback by ending episode with residual=0
                        self.comm.send_residual(0.0, False, True)
                        print("Comms degraded â†’ advise PI-only fallback on Robot PC; ending episode.")
                        break
                else:
                    self.fail_count = 0
                    episode_packets_sent += 1
                    
                    # 50Hz ì „ì†¡ ëª¨ë‹ˆí„°ë§ (1ì´ˆë§ˆë‹¤)
                    current_time = time.time()
                    if int(current_time) % 1 == 0 and int(current_time - episode_start_time) % 1 == 0:
                        print(f"ğŸ“¡ 50Hz Send: residual={rl_residual:.3f}MPa, episode_done={episode_done}, timing_ok={timing_ok}")

                # ğŸ¯ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ì¦‰ì‹œ break
                if episode_done:
                    print(f"ğŸ¯ Episode {ep+1} completed at step {self.episode_step:,}")
                    
                    # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ëª…í™•í•œ ì‹ í˜¸ ì „ì†¡
                    print(f"ğŸ“¡ Sending final episode_done=True for episode {ep+1}")
                    self.comm.send_residual(0.0, True, True)
                    
                    break

                # (4) logging (light) - 2.5ì´ˆë§ˆë‹¤ ë¡œê¹…
                current_time = time.time()
                if (self.last_log_time is None or 
                    current_time - self.last_log_time >= 2.5):  # 2.5ì´ˆë§ˆë‹¤ ë¡œê¹…
                    mode = "RESIDUAL" if rl_flag else "PI-ONLY"
                    
                    # ğŸ¯ ëª©í‘œ ì ‘ì´‰ë ¥ ë‹¬ì„± ì‹œ íŠ¹ë³„ í‘œì‹œ
                    force_achieved = ""
                    if abs(state[0] - state[1]) < 0.5:  # ëª©í‘œ Â±0.5N ë‹¬ì„±
                        force_achieved = " ğŸ¯ TARGET ACHIEVED!"
                    
                    print(f"[Ep {ep+1}/10] Step {self.episode_step} | {mode} | "
                          f"F {state[0]:.1f}/{state[1]:.1f}N | "
                          f"PI {state[5]:.3f}MPa | RL {rl_residual:.3f}MPa | "
                          f"{'EXACT' if timing_ok else 'DELAY'} | "
                          f"Time: {current_time - episode_start_time:.1f}s | "
                          f"RL_Flag: {rl_flag}{force_achieved}")
                    self.last_log_time = current_time
                self.control_updates += 1

                # (5) keep for next transition
                prev_state = state.copy()
                prev_action = rl_residual
                prev_rl_flag = rl_flag

                # ğŸ¯ ì—í”¼ì†Œë“œ ì¢…ë£ŒëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨ (episode_done ì²´í¬)
                # ì¤‘ë³µëœ break ë¡œì§ ì œê±°

            # ---- episode end ----
            episode_duration = time.time() - episode_start_time
            
            # ì—í”¼ì†Œë“œ í†µê³„ ì €ì¥
            episode_stat = {
                "episode": ep + 1,
                "duration": episode_duration,
                "steps": self.episode_step,
                "reward": self.current_episode_reward,
                "packets_received": episode_packets_received,
                "packets_sent": episode_packets_sent,
                "rl_active_steps": episode_rl_active_steps,
                "rl_active_ratio": episode_rl_active_steps / self.episode_step if self.episode_step > 0 else 0
            }
            episode_stats.append(episode_stat)
            
            self.agent.episode_rewards.append(self.current_episode_reward)
            if self.current_episode_reward > self.best_episode_reward:
                self.best_episode_reward = self.current_episode_reward
                self.best_agent_episode = ep
                self.agent.save_model(f"{self.cfg['MODEL_SAVE_DIR']}/test_best_agent_episode_{ep+1}_reward_{self.best_episode_reward:.2f}.pth")

            # dynamic threshold saving (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë” ìì£¼)
            if ep % self.cfg["SAVE_THRESH_FREQ"] == 0 and ep > 0:
                recent = self.agent.episode_rewards[-self.cfg["SAVE_THRESH_FREQ"]:]
                th = np.percentile(recent, self.cfg["SAVE_THRESH_PCT"])
                if self.current_episode_reward >= th:
                    self.agent.save_model(f"{self.cfg['MODEL_SAVE_DIR']}/test_high_perf_ep_{ep+1}_reward_{self.current_episode_reward:.2f}.pth")

            # ì—í”¼ì†Œë“œ ì™„ë£Œ ìš”ì•½
            print(f"\nğŸ¯ === EPISODE {ep+1}/10 COMPLETED ===")
            print(f"â±ï¸  Duration: {episode_duration:.1f}s")
            print(f"ğŸ“Š Steps: {self.episode_step:,}/{self.max_episode_steps:,} ({self.episode_step/self.max_episode_steps*100:.1f}%)")
            print(f"ğŸ† Reward: {self.current_episode_reward:.2f}")
            print(f"ğŸ“¥ Packets Received: {episode_packets_received}")
            print(f"ğŸ“¤ Packets Sent: {episode_packets_sent}")
            print(f"ğŸ¤– RL Active Steps: {episode_rl_active_steps} ({episode_stat['rl_active_ratio']*100:.1f}%)")
            print(f"ğŸ“ˆ Best So Far: {self.best_episode_reward:.2f}")
            
            # ğŸ¯ ì—í”¼ì†Œë“œ ì™„ë£Œ ì´ìœ  í‘œì‹œ
            if self.episode_step >= self.max_episode_steps:
                print(f"âœ… Completed: Reached maximum episode steps ({self.max_episode_steps:,})")
            else:
                print("âš ï¸  Completed: Episode terminated early (safety or error)")
            print("=" * 40)
            
            # í†µì‹  ìƒíƒœ ëª¨ë‹ˆí„°ë§ (5ì—í”¼ì†Œë“œë§ˆë‹¤)
            if (ep + 1) % 5 == 0:
                self.comm.print_communication_stats()

        # ---- ì „ì²´ í•™ìŠµ ì™„ë£Œ ----
        print("\nğŸ¯ TEST Training finished!")
        print(f"âœ… Completed {episodes} episodes successfully")
        print(f"ğŸ† Best Episode: {self.best_agent_episode+1}, Best Reward: {self.best_episode_reward:.2f}")
        print(f"ğŸ“ˆ Total RL active steps: {self.total_rl_active_steps}")
        
        # ì „ì²´ í†µê³„ ìš”ì•½
        print("\nğŸ“Š === FINAL TRAINING SUMMARY ===")
        total_duration = sum(ep["duration"] for ep in episode_stats)
        total_packets_received = sum(ep["packets_received"] for ep in episode_stats)
        total_packets_sent = sum(ep["packets_sent"] for ep in episode_stats)
        avg_reward = np.mean([ep["reward"] for ep in episode_stats])
        
        print(f"â±ï¸  Total Duration: {total_duration:.1f}s")
        print(f"ğŸ“¥ Total Packets Received: {total_packets_received}")
        print(f"ğŸ“¤ Total Packets Sent: {total_packets_sent}")
        print(f"ğŸ“Š Average Reward: {avg_reward:.2f}")
        print(f"ğŸ“ˆ Best Reward: {self.best_episode_reward:.2f}")
        print(f"ğŸ¤– Total RL Active Steps: {self.total_rl_active_steps}")
        
        # ì—í”¼ì†Œë“œë³„ ìƒì„¸ í†µê³„
        print("\nğŸ“‹ === EPISODE DETAILS ===")
        for ep_stat in episode_stats:
            print(f"Ep {ep_stat['episode']:2d}: "
                  f"Reward {ep_stat['reward']:6.2f}, "
                  f"Steps {ep_stat['steps']:4d}, "
                  f"RL Active {ep_stat['rl_active_steps']:4d} "
                  f"({ep_stat['rl_active_ratio']*100:4.1f}%)")
        
        # ìµœì¢… í†µì‹  í†µê³„
        self.comm.print_communication_stats()
        
        self.comm.close()

# =========================
# Main - TEST VERSION
# =========================
if __name__ == "__main__":
    print("ğŸ§ª TEST VERSION: JY_Pneumatic_SAC_Pre_only_test.py")
    print("=" * 60)
    
    set_seed(42)
    env = PneumaticPolishingEnvironment(CONFIG)
    
    try:
        print("ğŸš€ Starting test training...")
        env.run_training(CONFIG["EPISODES"])
        print("âœ… Test completed successfully!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted by user.")
        env.comm.close()
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        env.comm.close()
        
    finally:
        print("ğŸ”š Test program terminated.")
